{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sxQ1JblUkFcV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']\n",
        "\n",
        "for pt, en in train_examples.take(3):\n",
        "    print(f'Português: {pt.numpy().decode(\"utf-8\")}')\n",
        "    print(f'Inglês: {en.numpy().decode(\"utf-8\")}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmVeQJzpkHGE",
        "outputId": "006247cb-0cda-4e04-8266-b726b279416e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Português: e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "Inglês: and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "\n",
            "Português: mas e se estes fatores fossem ativos ?\n",
            "Inglês: but what if it were active ?\n",
            "\n",
            "Português: mas eles não tinham a curiosidade de me testar .\n",
            "Inglês: but they did n't test for curiosity .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (pt.numpy() for pt, _ in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for _, en in train_examples), target_vocab_size=2**13)"
      ],
      "metadata": {
        "id": "UQusvbg-kJb_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(lang1, lang2):\n",
        "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
        "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "    return lang1, lang2\n",
        "\n",
        "def tf_encode(pt, en):\n",
        "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
        "    result_pt.set_shape([None])\n",
        "    result_en.set_shape([None])\n",
        "    return result_pt, result_en\n",
        "\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(lambda x, y: tf.logical_and(tf.size(x) <= 40, tf.size(y) <= 40))\n",
        "train_dataset = train_dataset.cache().shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = val_examples.map(tf_encode)\n",
        "val_dataset = val_dataset.filter(lambda x, y: tf.logical_and(tf.size(x) <= 40, tf.size(y) <= 40))\n",
        "val_dataset = val_dataset.padded_batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "6vtU3hyqrsQg"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(\n",
        "        10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0,2,1,3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        q = self.split_heads(self.wq(q))\n",
        "        k = self.split_heads(self.wk(k))\n",
        "        v = self.split_heads(self.wv(v))\n",
        "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0,2,1,3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (tf.shape(scaled_attention)[0], -1, self.num_heads * self.depth))\n",
        "        return self.dense(concat_attention)\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'),\n",
        "                                tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, *, training=False, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(1000, d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, *, training=False, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, *, training=False,\n",
        "             look_ahead_mask=None, padding_mask=None):\n",
        "        attn1 = self.mha1(x, x, x, mask=look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2 = self.mha2(enc_output, enc_output, out1, mask=padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 target_vocab_size, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(1000, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, *, training=False,\n",
        "             look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.dec_layers[i](x, enc_output, training=training,\n",
        "                                   look_ahead_mask=look_ahead_mask,\n",
        "                                   padding_mask=padding_mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, target_vocab_size, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                               input_vocab_size, rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                               target_vocab_size, rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, *, training=False):\n",
        "        enc_padding_mask = create_padding_mask(inp)\n",
        "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "        dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
        "\n",
        "        dec_output = self.decoder(\n",
        "            tar, enc_output, training=training,\n",
        "            look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output"
      ],
      "metadata": {
        "id": "tyn8i4KCrv_j"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.96)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "aHGU9dWArv-I"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "num_layers = 2\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
        "target_vocab_size = tokenizer_en.vocab_size + 2\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS}')\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        tar_inp = tar[:, :-1]\n",
        "        tar_real = tar[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(inp, tar_inp, training=True)\n",
        "            loss = loss_function(tar_real, predictions)\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        if batch % 50 == 0:\n",
        "            print(f'Batch {batch} Loss {loss.numpy():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRkR-Ltnr6GO",
        "outputId": "bdf4f589-5434-493b-ebff-889a594f07f1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Batch 0 Loss 9.0268\n",
            "Batch 50 Loss 6.4374\n",
            "Batch 100 Loss 5.5710\n",
            "Batch 150 Loss 5.2218\n",
            "Batch 200 Loss 4.9434\n",
            "Batch 250 Loss 4.9428\n",
            "Batch 300 Loss 4.7747\n",
            "Batch 350 Loss 4.5717\n",
            "Batch 400 Loss 4.3790\n",
            "Batch 450 Loss 4.7369\n",
            "Batch 500 Loss 4.3430\n",
            "Batch 550 Loss 4.1401\n",
            "Batch 600 Loss 4.1599\n",
            "Batch 650 Loss 3.8911\n",
            "Batch 700 Loss 4.0951\n",
            "Epoch 2/10\n",
            "Batch 0 Loss 4.0192\n",
            "Batch 50 Loss 3.7211\n",
            "Batch 100 Loss 3.6934\n",
            "Batch 150 Loss 3.7875\n",
            "Batch 200 Loss 3.8874\n",
            "Batch 250 Loss 3.7133\n",
            "Batch 300 Loss 3.5979\n",
            "Batch 350 Loss 3.4689\n",
            "Batch 400 Loss 3.7451\n",
            "Batch 450 Loss 3.4755\n",
            "Batch 500 Loss 3.3914\n",
            "Batch 550 Loss 3.3096\n",
            "Batch 600 Loss 3.1531\n",
            "Batch 650 Loss 3.3774\n",
            "Batch 700 Loss 3.1614\n",
            "Epoch 3/10\n",
            "Batch 0 Loss 2.9643\n",
            "Batch 50 Loss 3.0197\n",
            "Batch 100 Loss 3.2052\n",
            "Batch 150 Loss 3.1533\n",
            "Batch 200 Loss 3.0864\n",
            "Batch 250 Loss 3.1932\n",
            "Batch 300 Loss 2.8096\n",
            "Batch 350 Loss 2.9331\n",
            "Batch 400 Loss 2.9592\n",
            "Batch 450 Loss 2.9092\n",
            "Batch 500 Loss 2.9230\n",
            "Batch 550 Loss 2.9784\n",
            "Batch 600 Loss 2.9060\n",
            "Batch 650 Loss 2.8460\n",
            "Batch 700 Loss 2.6901\n",
            "Epoch 4/10\n",
            "Batch 0 Loss 2.5038\n",
            "Batch 50 Loss 2.5447\n",
            "Batch 100 Loss 2.6202\n",
            "Batch 150 Loss 2.5591\n",
            "Batch 200 Loss 2.4516\n",
            "Batch 250 Loss 2.3492\n",
            "Batch 300 Loss 2.4158\n",
            "Batch 350 Loss 2.4074\n",
            "Batch 400 Loss 2.3714\n",
            "Batch 450 Loss 2.2782\n",
            "Batch 500 Loss 2.4559\n",
            "Batch 550 Loss 2.2928\n",
            "Batch 600 Loss 2.5414\n",
            "Batch 650 Loss 2.2905\n",
            "Batch 700 Loss 2.2679\n",
            "Epoch 5/10\n",
            "Batch 0 Loss 2.2046\n",
            "Batch 50 Loss 2.0002\n",
            "Batch 100 Loss 2.1631\n",
            "Batch 150 Loss 2.1027\n",
            "Batch 200 Loss 1.9921\n",
            "Batch 250 Loss 2.1799\n",
            "Batch 300 Loss 1.9893\n",
            "Batch 350 Loss 2.0894\n",
            "Batch 400 Loss 2.2211\n",
            "Batch 450 Loss 1.7958\n",
            "Batch 500 Loss 2.1803\n",
            "Batch 550 Loss 2.2524\n",
            "Batch 600 Loss 2.1786\n",
            "Batch 650 Loss 2.0785\n",
            "Batch 700 Loss 1.9302\n",
            "Epoch 6/10\n",
            "Batch 0 Loss 1.7552\n",
            "Batch 50 Loss 1.8479\n",
            "Batch 100 Loss 1.9448\n",
            "Batch 150 Loss 2.1123\n",
            "Batch 200 Loss 2.0060\n",
            "Batch 250 Loss 1.8004\n",
            "Batch 300 Loss 1.8286\n",
            "Batch 350 Loss 1.8447\n",
            "Batch 400 Loss 1.8834\n",
            "Batch 450 Loss 1.7438\n",
            "Batch 500 Loss 1.8181\n",
            "Batch 550 Loss 1.9399\n",
            "Batch 600 Loss 2.0424\n",
            "Batch 650 Loss 1.9895\n",
            "Batch 700 Loss 1.8963\n",
            "Epoch 7/10\n",
            "Batch 0 Loss 1.5527\n",
            "Batch 50 Loss 1.5213\n",
            "Batch 100 Loss 1.7207\n",
            "Batch 150 Loss 1.7603\n",
            "Batch 200 Loss 1.6209\n",
            "Batch 250 Loss 1.6304\n",
            "Batch 300 Loss 1.7608\n",
            "Batch 350 Loss 1.8604\n",
            "Batch 400 Loss 1.8147\n",
            "Batch 450 Loss 1.6394\n",
            "Batch 500 Loss 1.7265\n",
            "Batch 550 Loss 1.5955\n",
            "Batch 600 Loss 1.7698\n",
            "Batch 650 Loss 1.7551\n",
            "Batch 700 Loss 1.7177\n",
            "Epoch 8/10\n",
            "Batch 0 Loss 1.7189\n",
            "Batch 50 Loss 1.5433\n",
            "Batch 100 Loss 1.6272\n",
            "Batch 150 Loss 1.6651\n",
            "Batch 200 Loss 1.4285\n",
            "Batch 250 Loss 1.6222\n",
            "Batch 300 Loss 1.6945\n",
            "Batch 350 Loss 1.7122\n",
            "Batch 400 Loss 1.4445\n",
            "Batch 450 Loss 1.6560\n",
            "Batch 500 Loss 1.5228\n",
            "Batch 550 Loss 1.6934\n",
            "Batch 600 Loss 1.7062\n",
            "Batch 650 Loss 1.7965\n",
            "Batch 700 Loss 1.7675\n",
            "Epoch 9/10\n",
            "Batch 0 Loss 1.4600\n",
            "Batch 50 Loss 1.2511\n",
            "Batch 100 Loss 1.6001\n",
            "Batch 150 Loss 1.3843\n",
            "Batch 200 Loss 1.5422\n",
            "Batch 250 Loss 1.4173\n",
            "Batch 300 Loss 1.4662\n",
            "Batch 350 Loss 1.6586\n",
            "Batch 400 Loss 1.4230\n",
            "Batch 450 Loss 1.6126\n",
            "Batch 500 Loss 1.7031\n",
            "Batch 550 Loss 1.7681\n",
            "Batch 600 Loss 1.6465\n",
            "Batch 650 Loss 1.7390\n",
            "Batch 700 Loss 1.5765\n",
            "Epoch 10/10\n",
            "Batch 0 Loss 1.4702\n",
            "Batch 50 Loss 1.3645\n",
            "Batch 100 Loss 1.3663\n",
            "Batch 150 Loss 1.3060\n",
            "Batch 200 Loss 1.4634\n",
            "Batch 250 Loss 1.2526\n",
            "Batch 300 Loss 1.5982\n",
            "Batch 350 Loss 1.4053\n",
            "Batch 400 Loss 1.3178\n",
            "Batch 450 Loss 1.4317\n",
            "Batch 500 Loss 1.4121\n",
            "Batch 550 Loss 1.3252\n",
            "Batch 600 Loss 1.4696\n",
            "Batch 650 Loss 1.3545\n",
            "Batch 700 Loss 1.5038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    sentence = tf.expand_dims(\n",
        "        [tokenizer_pt.vocab_size] + tokenizer_pt.encode(sentence) + [tokenizer_pt.vocab_size+1],\n",
        "        axis=0\n",
        "    )\n",
        "    output = tf.expand_dims([tokenizer_en.vocab_size], 0)\n",
        "\n",
        "    for i in range(40):\n",
        "        predictions = transformer(sentence, output, training=False)\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == tokenizer_en.vocab_size+1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0)\n",
        "\n",
        "def translate(sentence):\n",
        "    result = evaluate(sentence)\n",
        "    predicted_sentence = tokenizer_en.decode([i for i in result.numpy() if i < tokenizer_en.vocab_size])\n",
        "    print(f'Input: {sentence}')\n",
        "    print(f'Predicted translation: {predicted_sentence}')"
      ],
      "metadata": {
        "id": "Agd0HupVr-9R"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"este é um problema que temos que resolver.\")\n",
        "translate(\"os meus vizinhos ouviram sobre essa ideia.\")\n",
        "translate(\"vou então muito rapidamente partilhar algumas histórias de algumas coisas mágicas que aconteceram.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76U4KxOwTBMK",
        "outputId": "377bca1d-6e23-442f-d2e3-56ea33932e89"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: este é um problema que temos que resolver.\n",
            "Predicted translation: this is a problem we have to fix that we solve .\n",
            "Input: os meus vizinhos ouviram sobre essa ideia.\n",
            "Predicted translation: my neighbors heard about that idea .\n",
            "Input: vou então muito rapidamente partilhar algumas histórias de algumas coisas mágicas que aconteceram.\n",
            "Predicted translation: so i 'm going to share some very short stories of some magic stories that happened to happen .\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}